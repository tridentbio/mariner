{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e2f7367-a2c0-4a26-a3ea-4bafd4029739",
   "metadata": {},
   "source": [
    "# Documentation introspection Aalysis\n",
    "\n",
    "1. Get the documentation URL:\n",
    "    This can be done by using the documentation path from the lib. I the case of pytorch, it is\n",
    "    `https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear`\n",
    "    and for pygnn it is `https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv`\n",
    "    In both cases we just need to subtitute the URL hash fragment in the end\n",
    "2. Get the docs strign andn remove unwanted sections from it, such as the Examples section\n",
    "3. Figure out how many inputs and outputs the graph editor node component will have "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c5fdf-04f7-49e7-8b3b-79ae77c591b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Get the documentation URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c2b4ac-d446-400a-9340-7bcd631580fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Sigmoid', 'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.ReLU', 'https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.GCNConv', None]\n"
     ]
    }
   ],
   "source": [
    "from app.features.model.generate import layers, featurizers\n",
    "\n",
    "def is_from_pygnn(class_path: str) -> bool:\n",
    "    return class_path.startswith('torch_geometric.')\n",
    "\n",
    "def is_from_pytorch(class_path: str) -> bool:\n",
    "    return class_path.startswith('torch.')\n",
    "\n",
    "def get_documentation_link(class_path: str) -> str:\n",
    "    if is_from_pygnn(class_path):\n",
    "        return f'https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#{class_path}'\n",
    "    elif is_from_pytorch(class_path):\n",
    "        return f'https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#{class_path}'\n",
    "    return None\n",
    "    \n",
    "print([ get_documentation_link(comp.name) for comp in layers + featurizers ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac6077-d113-4068-81e9-f999d7bd6b0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Get the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2bbda7-c08a-4699-81e8-78e686a68a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lib</th>\n",
       "      <th>docs</th>\n",
       "      <th>has_examples</th>\n",
       "      <th>has_args</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>torch_geometric.nn.GCNConv</th>\n",
       "      <td>pygnn</td>\n",
       "      <td>The graph convolutional operator from the `\"Se...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.nn.Linear</th>\n",
       "      <td>torch</td>\n",
       "      <td>Applies a linear transformation to the incomin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.nn.Sigmoid</th>\n",
       "      <td>torch</td>\n",
       "      <td>Applies the element-wise function:\\n\\n    .. m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>torch.nn.ReLU</th>\n",
       "      <td>torch</td>\n",
       "      <td>Applies the rectified linear unit function ele...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app.features.model.layers.GlobalPooling</th>\n",
       "      <td>app</td>\n",
       "      <td>A global pooling module that wraps the usage o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app.features.model.layers.Concat</th>\n",
       "      <td>app</td>\n",
       "      <td>\\n    A helper layer that concatenates the out...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app.features.model.featurizers.MoleculeFeaturizer</th>\n",
       "      <td>app</td>\n",
       "      <td>\\n    Small molecule featurizer.\\n    Args:\\n ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  src_lib  \\\n",
       "torch_geometric.nn.GCNConv                          pygnn   \n",
       "torch.nn.Linear                                     torch   \n",
       "torch.nn.Sigmoid                                    torch   \n",
       "torch.nn.ReLU                                       torch   \n",
       "app.features.model.layers.GlobalPooling               app   \n",
       "app.features.model.layers.Concat                      app   \n",
       "app.features.model.featurizers.MoleculeFeaturizer     app   \n",
       "\n",
       "                                                                                                docs  \\\n",
       "torch_geometric.nn.GCNConv                         The graph convolutional operator from the `\"Se...   \n",
       "torch.nn.Linear                                    Applies a linear transformation to the incomin...   \n",
       "torch.nn.Sigmoid                                   Applies the element-wise function:\\n\\n    .. m...   \n",
       "torch.nn.ReLU                                      Applies the rectified linear unit function ele...   \n",
       "app.features.model.layers.GlobalPooling            A global pooling module that wraps the usage o...   \n",
       "app.features.model.layers.Concat                   \\n    A helper layer that concatenates the out...   \n",
       "app.features.model.featurizers.MoleculeFeaturizer  \\n    Small molecule featurizer.\\n    Args:\\n ...   \n",
       "\n",
       "                                                   has_examples  has_args  \n",
       "torch_geometric.nn.GCNConv                                    0         1  \n",
       "torch.nn.Linear                                               1         1  \n",
       "torch.nn.Sigmoid                                              1         0  \n",
       "torch.nn.ReLU                                                 1         1  \n",
       "app.features.model.layers.GlobalPooling                       0         1  \n",
       "app.features.model.layers.Concat                              0         0  \n",
       "app.features.model.featurizers.MoleculeFeaturizer             0         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from app.features.model.generate import layers, featurizers\n",
    "from app.features.model.utils import get_class_from_path_string\n",
    "\n",
    "\n",
    "alldocs = [\n",
    "    (layer.name, get_class_from_path_string(layer.name).__doc__)\n",
    "    for layer in layers + featurizers\n",
    "]\n",
    "def get_by_first_start(tuples, first_start):\n",
    "    return [\n",
    "        item\n",
    "        for item in tuples\n",
    "        if item[0].startswith(first_start)\n",
    "    ]\n",
    "\n",
    "torch_geometric_comps = get_by_first_start(alldocs, 'torch_geometric.')\n",
    "torch_comps = get_by_first_start(alldocs, 'torch.')\n",
    "app_comps = get_by_first_start(alldocs, 'app.')\n",
    "\n",
    "def has_examples(docs):\n",
    "    return 'Examples:' in docs\n",
    "\n",
    "\n",
    "def has_args(docs):\n",
    "    return 'Args:\\n' in docs\n",
    "\n",
    "def has_docs(docs):\n",
    "    return docs is not None and len(docs) > 0\n",
    "\n",
    "df = pd.DataFrame({ 'src_lib': [], 'docs': [], 'has_examples': [], 'has_args': [], 'has_examples': [] })\n",
    "def info(title, stuff, df):\n",
    "    for (class_path, docs) in stuff:\n",
    "        df.loc[class_path] = {\n",
    "            'docs': docs,\n",
    "            'src_lib': title,\n",
    "            'has_args': int(has_args(docs)),\n",
    "            'has_examples': int(has_examples(docs))\n",
    "        }\n",
    "    \n",
    "info('pygnn', torch_geometric_comps, df)\n",
    "info('torch', torch_comps, df)\n",
    "info('app', app_comps, df)\n",
    "df.loc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2c5bc4-0b6c-46e0-817c-a9b1a81e0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
      "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
      "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
      "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      "                If :attr:`bias` is ``True``, the values are initialized from\n",
      "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = torch.randn(128, 20)\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "        torch.Size([128, 30])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(df.loc['torch.nn.Linear', 'docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820f8a4-a4d6-435a-a003-b3e4daef2ace",
   "metadata": {},
   "source": [
    "### 2.1 Removing Sections of the docs\n",
    "To make the doc. simpler, we need to remove sections that refer exclusivelly to coding such as the Examples section\n",
    "\n",
    "As can be seen, documentation is divided by indentation blocks.\n",
    "\n",
    "So an ideia to remove sections is to find it's title, and the next indentation block. then slice it off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9743510-ea8c-4d91-9891-ce51c686c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph convolutional operator from the `\"Semi-supervised\n",
      "    Classification with Graph Convolutional Networks\"\n",
      "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
      "\n",
      "    .. math::\n",
      "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
      "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
      "\n",
      "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
      "    adjacency matrix with inserted self-loops and\n",
      "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
      "    The adjacency matrix can include other values than :obj:`1` representing\n",
      "    edge weights via the optional :obj:`edge_weight` tensor.\n",
      "\n",
      "    Its node-wise formulation is given by:\n",
      "\n",
      "    .. math::\n",
      "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}^{\\top} \\sum_{j \\in\n",
      "        \\mathcal{N}(v) \\cup \\{ i \\}} \\frac{e_{j,i}}{\\sqrt{\\hat{d}_j\n",
      "        \\hat{d}_i}} \\mathbf{x}_j\n",
      "\n",
      "    with :math:`\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)} e_{j,i}`, where\n",
      "    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n",
      "    node :obj:`i` (default: :obj:`1.0`)\n",
      "\n",
      "    Args:\n",
      "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
      "            the size from the first input(s) to the forward method.\n",
      "        out_channels (int): Size of each output sample.\n",
      "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
      "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
      "            (default: :obj:`False`)\n",
      "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
      "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
      "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
      "            cached version for further executions.\n",
      "            This parameter should only be set to :obj:`True` in transductive\n",
      "            learning scenarios. (default: :obj:`False`)\n",
      "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
      "            self-loops to the input graph. (default: :obj:`True`)\n",
      "        normalize (bool, optional): Whether to add self-loops and compute\n",
      "            symmetric normalization coefficients on the fly.\n",
      "            (default: :obj:`True`)\n",
      "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
      "            an additive bias. (default: :obj:`True`)\n",
      "        **kwargs (optional): Additional arguments of\n",
      "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
      "\n",
      "    Shapes:\n",
      "        - **input:**\n",
      "          node features :math:`(|\\mathcal{V}|, F_{in})`,\n",
      "          edge indices :math:`(2, |\\mathcal{E}|)`,\n",
      "          edge weights :math:`(|\\mathcal{E}|)` *(optional)*\n",
      "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})`\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(df.loc['torch_geometric.nn.GCNConv', 'docs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74046c1-4d33-458a-bcae-b93fa708bf5a",
   "metadata": {},
   "source": [
    "### 2.2 Parse the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49dbdf9-faf8-4061-8363-782334cdc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_tabs(line: str):\n",
    "    # naive loop over chars\n",
    "    total = 0\n",
    "    for c in line:\n",
    "        if c == ' ' or c == '\\t':\n",
    "            total += 1\n",
    "        else:\n",
    "            break\n",
    "    return total\n",
    "\n",
    "def remove_indentation_of_section(text: str, section_title: str) -> str:\n",
    "    lines = text.split('\\n')\n",
    "    start_idx = None\n",
    "    tab_size = None\n",
    "    end_idx = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if section_title in line:\n",
    "            start_idx = idx\n",
    "            end_idx = start_idx\n",
    "            tab_size = count_tabs(line) + 4\n",
    "            continue\n",
    "        if start_idx is not None and count_tabs(line) >= tab_size:\n",
    "            end_idx += 1\n",
    "        elif start_idx is not None:\n",
    "            break\n",
    "    if start_idx is None:\n",
    "        return text\n",
    "    return '\\n'.join(lines[:start_idx] + lines[end_idx+1:])\n",
    "def test_remove_indentation():\n",
    "    assert remove_indentation_of_section(r'''The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classification with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "    The adjacency matrix can include other values than :obj:`1` representing\n",
    "    edge weights via the optional :obj:`edge_weight` tensor.\n",
    "\n",
    "    Its node-wise formulation is given by:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}^{\\top} \\sum_{j \\in\n",
    "        \\mathcal{N}(v) \\cup \\{ i \\}} \\frac{e_{j,i}}{\\sqrt{\\hat{d}_j\n",
    "        \\hat{d}_i}} \\mathbf{x}_j\n",
    "\n",
    "    with :math:`\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)} e_{j,i}`, where\n",
    "    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n",
    "    node :obj:`i` (default: :obj:`1.0`)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
    "            cached version for further executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        normalize (bool, optional): Whether to add self-loops and compute\n",
    "            symmetric normalization coefficients on the fly.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:''', 'Args') == r'''The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classification with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "    The adjacency matrix can include other values than :obj:`1` representing\n",
    "    edge weights via the optional :obj:`edge_weight` tensor.\n",
    "\n",
    "    Its node-wise formulation is given by:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}^{\\top} \\sum_{j \\in\n",
    "        \\mathcal{N}(v) \\cup \\{ i \\}} \\frac{e_{j,i}}{\\sqrt{\\hat{d}_j\n",
    "        \\hat{d}_i}} \\mathbf{x}_j\n",
    "\n",
    "    with :math:`\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)} e_{j,i}`, where\n",
    "    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n",
    "    node :obj:`i` (default: :obj:`1.0`)\n",
    "\n",
    "\n",
    "    Shapes:'''\n",
    "    \n",
    "test_remove_indentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ccc33-0d70-4585-80f1-e3e34c80afa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Get info on the inputs and outputs of layer\n",
    "\n",
    "The actual information we can use might be more than what I'm selecting right now.\n",
    "\n",
    "### The Model Editor (ME)\n",
    "\n",
    "The editor consists of a wrapper of the [react-flow library](https://reactflow.dev/)\n",
    "along with other components to configure the dataset used for training and validation.\n",
    "\n",
    "Follows a textual description of it, although the editing experience should be familiar\n",
    "to those that have ever used a diagram flow editing tool:\n",
    "\n",
    "ME is intended to build the model's architecture, which corresponds to the layers field\n",
    "in [our model schema]() as well as the preprocessing of the inputs given to it during\n",
    "training and prediction, which corresponds to the featurizers field in the same schema.\n",
    "\n",
    "The initial state of the model editor are the configurations of a given `dataset`, namely\n",
    "which columns are gonna be used as features and targets. In the ME, this is shown as\n",
    "draggable nodes, each one labeled after the column it was originated from. Each node\n",
    "must have at least one endpoint, which can be a source endpoint or a target endpoint.\n",
    "The only connections allowed between nodes are from source endpoints to target ones.\n",
    "Inputs/outputs are nodes with a single source/target endpoint,\n",
    "\n",
    "Given this initial state, we can use ME options as building blocks for the rest\n",
    "of the model editing. Each option is selected from `mariner`, `pytorch` and `pygnn`\n",
    "libraries and may accept one or more inputs, and one or more output. For each input\n",
    "it should accept one connection in it's source endpoint, and for each output, it should\n",
    "require a connection from it's source endpoint.\n",
    "\n",
    "| component      | inputs | outputs |   special conditions                               |\n",
    "| -------------  | ------ | ------- | -------------------------------------------------- |\n",
    "| mariner.Concat |   2    |    1    |   inputs should be tensors of same dtype           |\n",
    "| torch.Linear   |   1    |    1    |   input should be Tensors                          |\n",
    "| torch.GCN      |   1    |    1    |   inputs should be graph featurized smiles columns |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6031f-45a2-4532-b763-1463ef50e4d2",
   "metadata": {},
   "source": [
    "### How type hints may help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1f537c-3002-44d8-986a-06d3e0d288fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Linear type hints:'\n",
      "{'input': <class 'torch.Tensor'>, 'return': <class 'torch.Tensor'>}\n",
      "'GCNConv type hints:'\n",
      "{'edge_index': typing.Union[torch.Tensor, torch_sparse.tensor.SparseTensor],\n",
      " 'edge_weight': typing.Optional[torch.Tensor],\n",
      " 'return': <class 'torch.Tensor'>,\n",
      " 'x': <class 'torch.Tensor'>}\n",
      "'Concat type hints:'\n",
      "{'x1': <class 'torch.Tensor'>, 'x2': <class 'torch.Tensor'>}\n",
      "'GlobalPooling type hints:'\n",
      "{'batch': typing.Optional[torch.Tensor],\n",
      " 'return': <class 'torch.Tensor'>,\n",
      " 'size': typing.Optional[int],\n",
      " 'x': <class 'torch.Tensor'>}\n"
     ]
    }
   ],
   "source": [
    "from typing import get_type_hints\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import torch_geometric\n",
    "from app.features.model import layers as mariner\n",
    "\n",
    "linear_type_hints = get_type_hints(torch.nn.Linear.forward)\n",
    "gcn_type_hints = get_type_hints(torch_geometric.nn.GCNConv.forward)\n",
    "concat_type_hints = get_type_hints(mariner.Concat.forward)\n",
    "global_pooling_type_hints = get_type_hints(mariner.GlobalPooling.forward)\n",
    "# Each type hints is a dictionory where the keys\n",
    "# are the method argument names and a special 'return'\n",
    "# and the value is the detected types for each arg and the\n",
    "# return type for 'return' key\n",
    "\n",
    "pprint('Linear type hints:')\n",
    "pprint(linear_type_hints)\n",
    "pprint('GCNConv type hints:')\n",
    "pprint(gcn_type_hints)\n",
    "pprint('Concat type hints:')\n",
    "pprint(concat_type_hints)\n",
    "pprint('GlobalPooling type hints:')\n",
    "pprint(global_pooling_type_hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa967b3-de39-4adb-b5c8-5e6fb27f17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "MessagePassingRule = Literal['graph-receiver']\n",
    "InputsSameTypeRule = Literal['inputs-same-type']\n",
    "LayerRule = Union[\n",
    "    MessagePassingRule,\n",
    "    InputsSameTypeRule\n",
    "]\n",
    "\n",
    "\n",
    "def get_inputs_outputs_and_rules(component_cls) -> tuple[int, int, List[LayerRule]]:\n",
    "    rules = []\n",
    "    if (\n",
    "        issubclass(component_cls, torch_geometric.nn.MessagePassing) or\n",
    "        issubclass(component_cls, mariner.GlobalPooling)\n",
    "    ):\n",
    "        rules.append('graph-receiver')\n",
    "        return 1, 1, rules\n",
    "    elif issubclass(component_cls, mariner.Concat):\n",
    "        rules.append('inputs-same-type')\n",
    "        return 2, 1, rules\n",
    "    elif issubclass(component_cls, torch.nn.Module):\n",
    "        type_hints_keys = get_type_hints(component_cls.forward).keys()\n",
    "        inputs = len(type_hints_keys) - int('return' in type_hints_keys)\n",
    "        return inputs, 1, rules\n",
    "    else:\n",
    "        type_hints_keys = get_type_hints(component_cls.__call__).keys()\n",
    "        inputs = len(type_hints_keys) - int('return' in type_hints_keys)\n",
    "        return inputs, 1, rules\n",
    "    \n",
    "assert get_inputs_outputs_and_rules(torch.nn.Linear) == (1, 1, [])\n",
    "assert get_inputs_outputs_and_rules(torch_geometric.nn.GCNConv) == (1, 1, ['graph-receiver'])\n",
    "assert get_inputs_outputs_and_rules(mariner.Concat) == (2, 1, ['inputs-same-type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ef5cf-a7fe-443e-813d-0f0794de91bb",
   "metadata": {},
   "source": [
    "## All Together Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a56d9f9-ca67-479c-8029-30dbefa0dc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerAnnotations(num_inputs=1, num_outputs=1, docs='Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\\n\\n    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\\n\\n    Args:\\n        in_features: size of each input sample\\n        out_features: size of each output sample\\n        bias: If set to ``False``, the layer will not learn an additive bias.\\n            Default: ``True``\\n\\n    Shape:\\n        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\\n          additional dimensions and :math:`H_{in} = \\\\text{in\\\\_features}`\\n        - Output: :math:`(N, *, H_{out})` where all but the last dimension\\n          are the same shape as the input and :math:`H_{out} = \\\\text{out\\\\_features}`.\\n\\n    Attributes:\\n        weight: the learnable weights of the module of shape\\n            :math:`(\\\\text{out\\\\_features}, \\\\text{in\\\\_features})`. The values are\\n            initialized from :math:`\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})`, where\\n            :math:`k = \\\\frac{1}{\\\\text{in\\\\_features}}`\\n        bias:   the learnable bias of the module of shape :math:`(\\\\text{out\\\\_features})`.\\n                If :attr:`bias` is ``True``, the values are initialized from\\n                :math:`\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})` where\\n                :math:`k = \\\\frac{1}{\\\\text{in\\\\_features}}`\\n\\n\\n        >>> m = nn.Linear(20, 30)\\n        >>> input = torch.randn(128, 20)\\n        >>> output = m(input)\\n        >>> print(output.size())\\n        torch.Size([128, 30])\\n    ', docs_link='https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear', rules=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class LayerAnnotations(BaseModel):\n",
    "    num_inputs: int\n",
    "    num_outputs: int\n",
    "    docs: str\n",
    "    docs_link: Optional[str] # not all layers have docs string just yet\n",
    "    rules: List[LayerRule]\n",
    "    \n",
    "def get_annotations_from_cls(cls_path: str) -> LayerAnnotations:\n",
    "    docs_link = get_documentation_link(cls_path)\n",
    "    cls = get_class_from_path_string(cls_path)\n",
    "    docs = remove_indentation_of_section(cls.__doc__, 'Examples:')\n",
    "    inputs, outputs, rules = get_inputs_outputs_and_rules(cls)\n",
    "    return LayerAnnotations(pread\n",
    "        docs_link=docs_link,\n",
    "        docs=docs,\n",
    "        num_inputs=inputs,\n",
    "        num_outputs=outputs,\n",
    "        rules=rules\n",
    "    )\n",
    "\n",
    "for component in layers + featurizers:\n",
    "    assert get_annotations_from_cls(component.name)\n",
    "\n",
    "get_annotations_from_cls('torch.nn.Linear')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
