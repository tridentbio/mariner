torch.nn.TransformerEncoderLayer:
  args_options:
    activation:
      - relu
      - sigmoid
  defaults:
    activation: relu

torch.nn.Embedding:
  defaults:
    max_norm: 1
    norm_type: 2
